{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from decimal import Decimal, getcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_assetTransferred_logs(row):\n",
    "    expanded_data = parse_decoded_input(row['decoded'])\n",
    "    if not expanded_data.empty:\n",
    "        return expanded_data\n",
    "    return pd.Series(dtype='float')  # Consistent empty return\n",
    "\n",
    "def parse_decoded_input(input_dict):\n",
    "    if input_dict.get('method_call', '').startswith('AssetTransferred'):\n",
    "        expanded_data = {'method_call': input_dict['method_call']}\n",
    "        # print(input_dict.get('parameters', []))\n",
    "        for param in input_dict.get('parameters', []):\n",
    "            expanded_data[param['name']] = param['value']\n",
    "        return pd.Series(expanded_data)\n",
    "    return pd.Series(dtype='float')\n",
    "\n",
    "def from_bn(val, dec=18):\n",
    "    # Set the precision for Decimal operations (optional, adjust as needed)\n",
    "    getcontext().prec = 50\n",
    "    \n",
    "    # Convert val to Decimal, assuming val can be cast directly\n",
    "    if isinstance(val, str):\n",
    "        val = Decimal(val)\n",
    "    elif not isinstance(val, Decimal):\n",
    "        val = Decimal(str(val))\n",
    "    \n",
    "    # Define the divisor as 10 raised to the power of dec\n",
    "    divisor = Decimal(10) ** dec\n",
    "    \n",
    "    # Perform the division\n",
    "    result = val / divisor\n",
    "    \n",
    "    # Convert the result to a string\n",
    "    return str(result)\n",
    "\n",
    "def sub_id_to_option_details(sub_id):\n",
    "    if isinstance(sub_id, str):\n",
    "        sub_id = int(sub_id)\n",
    "    # Define mask constants\n",
    "    UINT32_MAX = 0xffffffff\n",
    "    UINT63_MAX = 0x7fffffffffffffff\n",
    "\n",
    "    # Bitwise operations to extract values\n",
    "    expiry = sub_id & UINT32_MAX\n",
    "    strike = ((sub_id >> 32) & UINT63_MAX) * 10_000_000_000\n",
    "    is_call = (sub_id >> 95) > 0\n",
    "\n",
    "    # Return the details as a dictionary (could also use a custom class or namedtuple)\n",
    "    # expiry is a timestamp in seconds, convert it to a date object which is more readable\n",
    "    return dict(\n",
    "        expiry = pd.to_datetime(expiry, unit='s'),\n",
    "        strike = from_bn(strike),\n",
    "        type= 'call' if is_call else 'put'\n",
    "    )\n",
    "\n",
    "def extract_option_details(logs):\n",
    "    # Ensure the columns 'expiry', 'strike', and 'type' exist in the DataFrame\n",
    "    if 'expiry' not in logs.columns:\n",
    "        logs['expiry'] = None\n",
    "    if 'strike' not in logs.columns:\n",
    "        logs['strike'] = None\n",
    "    if 'type' not in logs.columns:\n",
    "        logs['type'] = None\n",
    "        \n",
    "    for index, row in logs.iterrows():\n",
    "        sub_id = row['subId']\n",
    "        if sub_id != '0':\n",
    "            option_details = sub_id_to_option_details(sub_id)\n",
    "            # attach option details['expiry'] to the row\n",
    "            logs.at[index, 'expiry'] = option_details['expiry']\n",
    "            logs.at[index, 'strike'] = option_details['strike']\n",
    "            logs.at[index, 'type'] = option_details['type']\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['fromAcc', 'toAcc', 'asset', 'subId', 'amount', 'tradeId', 'expiry', 'strike', 'type', 'block_number', 'index', 'tx_hash','method_call',]\n",
    "def fetch_all_logs(n, next_page_params, logs=pd.DataFrame()):    \n",
    "    url = 'https://explorer.lyra.finance/api/v2/addresses/0xE7603DF191D699d8BD9891b821347dbAb889E5a5/logs'\n",
    "    headers = {'accept': 'application/json'}\n",
    "    from_block = next_page_params['block_number']\n",
    "    for i in range(n):\n",
    "        response = requests.get(url, headers=headers, params=next_page_params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            batch = pd.DataFrame(data['items'])\n",
    "            # print(batch)\n",
    "            new_rows = batch.apply(filter_assetTransferred_logs, axis=1)\n",
    "            # Directly filter rows where new_rows are not empty before concatenating\n",
    "\n",
    "            # Ensure 'method_call' column exists in new_rows\n",
    "            if 'method_call' not in new_rows.columns:\n",
    "                new_rows['method_call'] = None\n",
    "\n",
    "            mask = new_rows['method_call'].notnull()\n",
    "            df_combined = batch[mask].join(new_rows[mask])  # Apply mask directly here\n",
    "\n",
    "            # Ensure the necessary columns exist in df_combined\n",
    "            for col in columns_to_keep:\n",
    "                if col not in df_combined.columns:\n",
    "                    df_combined[col] = None\n",
    "\n",
    "            parsed = extract_option_details(df_combined)[columns_to_keep]\n",
    "            logs = pd.concat([logs, parsed])\n",
    "            print(\"{}: Fetched {} asset transfer logs, next page {}\".format(i, len(parsed), next_page_params))\n",
    "            next_page_params = data['next_page_params']\n",
    "            c = next_page_params['block_number']\n",
    "            if(i%100 == 0):\n",
    "                print(f'saving {i}')   \n",
    "                logs.to_csv(f'./extracted/log-{from_block}-{c}.csv', index=False)\n",
    "        else:\n",
    "            print(\"Failed to fetch data:\", response.json())\n",
    "            fetch_all_logs(n, next_page_params)\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "max_retries = 100  # Maximum number of retries\n",
    "retry_delay = 3  # Delay between retries in seconds\n",
    "\n",
    "retry_count = 0\n",
    "successful = False\n",
    "\n",
    "while not successful and retry_count < max_retries:\n",
    "    try:\n",
    "        # Read all CSV files in the 'extracted' directory and concatenate them\n",
    "        sofar = pd.concat([pd.read_csv(f'extracted/{f}') for f in os.listdir('extracted') if f.endswith('.csv')])\n",
    "        sofar.drop_duplicates(inplace=True)\n",
    "        sofar.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Find the row with the minimum 'block_number'\n",
    "        last_block_index = sofar['block_number'].idxmin()\n",
    "        next_page_params = {\n",
    "            'block_number': sofar.iloc[last_block_index]['block_number'],\n",
    "            'index': sofar.iloc[last_block_index]['index'],\n",
    "            'items_count': 450\n",
    "        }\n",
    "\n",
    "        print('Starting from block:', next_page_params)\n",
    "\n",
    "        # 10000 is roughly a month\n",
    "        logs = fetch_all_logs(20000, next_page_params)\n",
    "        successful = True  # Set to True if everything was successful\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred: {e}')\n",
    "        retry_count += 1\n",
    "        print(f'Retrying... Attempt {retry_count}/{max_retries}')\n",
    "        time.sleep(retry_delay)  # Wait before retrying\n",
    "\n",
    "if not successful:\n",
    "    print(\"Failed to complete the process after several retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all CSV files in the 'extracted' directory and concatenate them\n",
    "all_extracted_logs = pd.concat([pd.read_csv(f'extracted/{f}') for f in os.listdir('extracted') if f.endswith('.csv')])\n",
    "all_extracted_logs.drop_duplicates(inplace=True)\n",
    "all_extracted_logs.reset_index(drop=True, inplace=True)\n",
    "last_block_index = all_extracted_logs.iloc[all_extracted_logs['block_number'].idxmin()]['block_number']\n",
    "first_block_index = all_extracted_logs.iloc[all_extracted_logs['block_number'].idxmax()]['block_number']\n",
    "# add them to blockkchain logs dir to process\n",
    "all_extracted_logs.to_csv(f'blockchain logs/transfers.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
